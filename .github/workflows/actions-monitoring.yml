name: GitHub Actions Monitoring

on:
  schedule:
    # Run every 6 hours to collect metrics
    - cron: "0 */6 * * *"
  workflow_dispatch:
    inputs:
      lookback_hours:
        description: 'Hours to look back for analysis'
        required: false
        default: '24'
        type: string

env:
  LOOKBACK_HOURS: ${{ github.event.inputs.lookback_hours || '24' }}

jobs:
  collect-metrics:
    name: Collect GitHub Actions Metrics
    runs-on: ubuntu-latest
    outputs:
      metrics: ${{ steps.analysis.outputs.metrics }}
      alert_needed: ${{ steps.analysis.outputs.alert_needed }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests python-dateutil

      - name: Collect workflow metrics
        id: analysis
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python3 << 'EOF'
          import requests
          import json
          import os
          from datetime import datetime, timedelta
          from dateutil import parser

          # Configuration
          REPO = "${{ github.repository }}"
          TOKEN = os.environ["GH_TOKEN"]
          LOOKBACK_HOURS = int(os.environ["LOOKBACK_HOURS"])

          headers = {
              "Authorization": f"Bearer {TOKEN}",
              "Accept": "application/vnd.github+json",
              "X-GitHub-Api-Version": "2022-11-28"
          }

          def get_workflow_runs():
              """Get workflow runs from the last N hours"""
              since = datetime.utcnow() - timedelta(hours=LOOKBACK_HOURS)
              url = f"https://api.github.com/repos/{REPO}/actions/runs"
              params = {
                  "per_page": 100,
                  "created": f">={since.isoformat()}Z"
              }

              response = requests.get(url, headers=headers, params=params)
              response.raise_for_status()
              return response.json()["workflow_runs"]

          def analyze_runs(runs):
              """Analyze workflow runs and generate metrics"""
              metrics = {
                  "total_runs": len(runs),
                  "success_count": 0,
                  "failure_count": 0,
                  "cancelled_count": 0,
                  "workflows": {},
                  "durations": [],
                  "queue_times": [],
                  "retry_count": 0
              }

              for run in runs:
                  workflow_name = run["name"]
                  conclusion = run["conclusion"]
                  status = run["status"]

                  # Initialize workflow metrics
                  if workflow_name not in metrics["workflows"]:
                      metrics["workflows"][workflow_name] = {
                          "total": 0,
                          "success": 0,
                          "failure": 0,
                          "cancelled": 0,
                          "avg_duration": 0,
                          "durations": []
                      }

                  metrics["workflows"][workflow_name]["total"] += 1

                  # Count by conclusion
                  if conclusion == "success":
                      metrics["success_count"] += 1
                      metrics["workflows"][workflow_name]["success"] += 1
                  elif conclusion == "failure":
                      metrics["failure_count"] += 1
                      metrics["workflows"][workflow_name]["failure"] += 1
                  elif conclusion == "cancelled":
                      metrics["cancelled_count"] += 1
                      metrics["workflows"][workflow_name]["cancelled"] += 1

                  # Calculate durations if completed
                  if run["updated_at"] and run["created_at"]:
                      created = parser.parse(run["created_at"])
                      updated = parser.parse(run["updated_at"])
                      duration = (updated - created).total_seconds()
                      metrics["durations"].append(duration)
                      metrics["workflows"][workflow_name]["durations"].append(duration)

                  # Count retries (run_attempt > 1)
                  if run["run_attempt"] > 1:
                      metrics["retry_count"] += 1

              # Calculate averages
              if metrics["durations"]:
                  metrics["avg_duration"] = sum(metrics["durations"]) / len(metrics["durations"])
                  metrics["p95_duration"] = sorted(metrics["durations"])[int(len(metrics["durations"]) * 0.95)] if len(metrics["durations"]) > 1 else metrics["durations"][0]

              for workflow in metrics["workflows"].values():
                  if workflow["durations"]:
                      workflow["avg_duration"] = sum(workflow["durations"]) / len(workflow["durations"])

              return metrics

          def check_alerts(metrics):
              """Check if any alerts should be triggered"""
              alerts = []

              # Calculate failure rate
              if metrics["total_runs"] > 0:
                  failure_rate = metrics["failure_count"] / metrics["total_runs"]
                  if failure_rate > 0.2:  # 20% failure rate threshold
                      alerts.append(f"High failure rate: {failure_rate:.1%}")

              # Check for excessive retries
              if metrics["total_runs"] > 0:
                  retry_rate = metrics["retry_count"] / metrics["total_runs"]
                  if retry_rate > 0.1:  # 10% retry rate threshold
                      alerts.append(f"High retry rate: {retry_rate:.1%}")

              # Check for slow workflows
              if metrics.get("p95_duration", 0) > 1800:  # 30 minutes
                  alerts.append(f"Slow workflows detected: P95 duration {metrics['p95_duration']/60:.1f} minutes")

              return alerts

          def format_summary(metrics, alerts):
              """Format a human-readable summary"""
              summary = f"""
          ## ðŸ“Š GitHub Actions Metrics (Last {LOOKBACK_HOURS}h)

          ### Overall Health
          - **Total Runs**: {metrics['total_runs']}
          - **Success Rate**: {metrics['success_count']}/{metrics['total_runs']} ({metrics['success_count']/max(metrics['total_runs'],1):.1%})
          - **Failures**: {metrics['failure_count']}
          - **Cancelled**: {metrics['cancelled_count']}
          - **Retries**: {metrics['retry_count']}

          ### Performance
          - **Average Duration**: {metrics.get('avg_duration', 0)/60:.1f} minutes
          - **P95 Duration**: {metrics.get('p95_duration', 0)/60:.1f} minutes

          ### By Workflow
          """

              for name, workflow in metrics["workflows"].items():
                  success_rate = workflow["success"] / max(workflow["total"], 1)
                  avg_duration = workflow["avg_duration"] / 60 if workflow["avg_duration"] else 0
                  summary += f"- **{name}**: {workflow['success']}/{workflow['total']} ({success_rate:.1%}) - Avg: {avg_duration:.1f}m\n"

              if alerts:
                  summary += "\n### ðŸš¨ Alerts\n"
                  for alert in alerts:
                      summary += f"- {alert}\n"
              else:
                  summary += "\n### âœ… No alerts - all metrics within normal range\n"

              return summary

          # Main execution
          try:
              print("Collecting workflow runs...")
              runs = get_workflow_runs()
              print(f"Found {len(runs)} runs in the last {LOOKBACK_HOURS} hours")

              print("Analyzing metrics...")
              metrics = analyze_runs(runs)

              print("Checking for alerts...")
              alerts = check_alerts(metrics)

              print("Generating summary...")
              summary = format_summary(metrics, alerts)

              # Output for GitHub Actions
              with open(os.environ["GITHUB_OUTPUT"], "a") as f:
                  f.write(f"metrics<<EOF\n{json.dumps(metrics)}\nEOF\n")
                  f.write(f"alert_needed={'true' if alerts else 'false'}\n")
                  f.write(f"summary<<EOF\n{summary}\nEOF\n")

              # Save detailed report
              with open("actions-metrics.json", "w") as f:
                  json.dump({
                      "timestamp": datetime.utcnow().isoformat(),
                      "lookback_hours": LOOKBACK_HOURS,
                      "metrics": metrics,
                      "alerts": alerts,
                      "summary": summary
                  }, f, indent=2)

              print("âœ… Metrics collection completed")
              print(summary)

          except Exception as e:
              print(f"âŒ Error collecting metrics: {e}")
              import traceback
              traceback.print_exc()
              exit(1)
          EOF

      - name: Upload metrics artifact
        uses: actions/upload-artifact@v4
        with:
          name: actions-metrics-${{ github.run_number }}
          path: actions-metrics.json
          retention-days: 30

      - name: Add metrics to job summary
        if: always()
        run: |
          if [ -f actions-metrics.json ]; then
            echo "$(jq -r '.summary // "No summary available"' actions-metrics.json)" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Failed to generate metrics" >> $GITHUB_STEP_SUMMARY
          fi

  alert-on-issues:
    name: Send Alerts
    runs-on: ubuntu-latest
    needs: collect-metrics
    if: needs.collect-metrics.outputs.alert_needed == 'true'
    steps:
      - name: Create alert issue
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const metrics = JSON.parse('${{ needs.collect-metrics.outputs.metrics }}');
            const title = `ðŸš¨ GitHub Actions Health Alert - ${new Date().toISOString().split('T')[0]}`;

            const body = `
            ## GitHub Actions Health Alert

            Automated monitoring has detected issues with our GitHub Actions workflows.

            ### Summary
            - **Time Range**: Last ${{ env.LOOKBACK_HOURS }} hours
            - **Total Runs**: ${metrics.total_runs}
            - **Failures**: ${metrics.failure_count}
            - **Success Rate**: ${((metrics.success_count / Math.max(metrics.total_runs, 1)) * 100).toFixed(1)}%
            - **Retries**: ${metrics.retry_count}

            ### Action Required
            - [ ] Review failed workflow runs
            - [ ] Check for infrastructure issues
            - [ ] Verify recent code changes
            - [ ] Update monitoring thresholds if needed

            ### Workflows Affected
            ${Object.entries(metrics.workflows).map(([name, data]) =>
              `- **${name}**: ${data.failure} failures out of ${data.total} runs`
            ).join('\n')}

            ---
            *This issue was automatically created by the Actions Monitoring workflow.*
            *Run ID: ${{ github.run_id }}*
            `;

            // Check if there's already an open alert issue
            const existingIssues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'actions-alert',
              per_page: 1
            });

            if (existingIssues.data.length > 0) {
              // Update existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssues.data[0].number,
                body: `## ðŸ“Š Updated Alert - ${new Date().toISOString()}

${body}`
              });
              console.log(`Updated existing alert issue #${existingIssues.data[0].number}`);
            } else {
              // Create new issue
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['actions-alert', 'monitoring', 'urgent']
              });
              console.log(`Created new alert issue #${issue.data.number}`);
            }

  historical-tracking:
    name: Update Historical Metrics
    runs-on: ubuntu-latest
    needs: collect-metrics
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download metrics
        uses: actions/download-artifact@v4
        with:
          name: actions-metrics-${{ github.run_number }}

      - name: Update metrics history
        run: |
          # Create metrics directory if it doesn't exist
          mkdir -p .github/metrics

          # Move current metrics to history
          if [ -f actions-metrics.json ]; then
            TIMESTAMP=$(date +%Y%m%d_%H%M%S)
            cp actions-metrics.json .github/metrics/actions-metrics-${TIMESTAMP}.json

            # Keep only last 30 days of metrics (roughly 120 files with 6h intervals)
            cd .github/metrics
            ls -t actions-metrics-*.json | tail -n +121 | xargs rm -f 2>/dev/null || true
            cd ../..

            # Create/update latest metrics
            cp actions-metrics.json .github/metrics/latest.json

            # Create simple CSV for trends
            if [ ! -f .github/metrics/trends.csv ]; then
              echo "timestamp,total_runs,success_count,failure_count,avg_duration,retry_count" > .github/metrics/trends.csv
            fi

            # Extract metrics and append to CSV
            python3 << 'EOF'
            import json
            import csv
            from datetime import datetime

            with open('actions-metrics.json', 'r') as f:
                data = json.load(f)

            metrics = data['metrics']
            row = [
                data['timestamp'],
                metrics['total_runs'],
                metrics['success_count'],
                metrics['failure_count'],
                round(metrics.get('avg_duration', 0), 2),
                metrics['retry_count']
            ]

            with open('.github/metrics/trends.csv', 'a', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(row)
            EOF

            echo "âœ… Updated metrics history"
          fi

      - name: Commit metrics history
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .github/metrics/
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Update Actions metrics history [skip ci]"
            git push
            echo "âœ… Committed metrics history"
          fi

  generate-badge:
    name: Update Success Rate Badge
    runs-on: ubuntu-latest
    needs: collect-metrics
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Generate badge
        run: |
          METRICS='${{ needs.collect-metrics.outputs.metrics }}'

          # Calculate success rate
          TOTAL=$(echo "$METRICS" | jq '.total_runs')
          SUCCESS=$(echo "$METRICS" | jq '.success_count')

          if [ "$TOTAL" -gt 0 ]; then
            RATE=$(echo "scale=1; $SUCCESS * 100 / $TOTAL" | bc -l)

            # Determine color
            if (( $(echo "$RATE >= 95" | bc -l) )); then
              COLOR="brightgreen"
            elif (( $(echo "$RATE >= 90" | bc -l) )); then
              COLOR="green"
            elif (( $(echo "$RATE >= 80" | bc -l) )); then
              COLOR="yellow"
            elif (( $(echo "$RATE >= 70" | bc -l) )); then
              COLOR="orange"
            else
              COLOR="red"
            fi

            mkdir -p .github/badges

            # Create badge JSON
            cat > .github/badges/actions-success-rate.json <<EOF
            {
              "schemaVersion": 1,
              "label": "actions success",
              "message": "${RATE}%",
              "color": "$COLOR"
            }
            EOF

            echo "Generated badge: ${RATE}% ($COLOR)"
          else
            echo "No runs to calculate success rate"
          fi

      - name: Commit badge
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add .github/badges/
          if git diff --staged --quiet; then
            echo "No badge changes to commit"
          else
            git commit -m "Update Actions success rate badge [skip ci]"
            git push
            echo "âœ… Updated badge"
          fi
